{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import glob \n",
    "import random\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import KLDivLoss\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, ConcatDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(<your_metadata.csv>)\n",
    "df = df[df['label']>0] # Including BI-RADS 1-5\n",
    "df['label'] = df['label']-1 # Rescoring to 0-4\n",
    "file_paths = df['name'].to_list()\n",
    "labels = df['label']\n",
    "labels = labels.to_list()\n",
    "print(file_paths)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_identifier_view(file_path):\n",
    "    filename = os.path.basename(file_path)\n",
    "    identifier, view = filename.split('-')[0], '-'.join(filename.split('-')[1:])\n",
    "    return identifier, view\n",
    "\n",
    "# Create a mapping from file paths to labels\n",
    "file_path_to_label = {file_path: label for file_path, label in zip(file_paths, labels)}\n",
    "\n",
    "# Grouping the file paths by identifier\n",
    "grouped_images = defaultdict(list)\n",
    "for file_path in file_paths:\n",
    "    identifier, view = extract_identifier_view(file_path)\n",
    "    grouped_images[identifier].append((view, file_path))\n",
    "\n",
    "# Pairing the images and creating new labels\n",
    "paired_images = []\n",
    "new_labels = []\n",
    "for identifier, images in grouped_images.items():\n",
    "    view_dict = {view: path for view, path in images}\n",
    "    if file_path_to_label[view_dict['L-CC.png']] >= 0:\n",
    "        if 'L-CC.png' in view_dict and 'L-MLO.png' in view_dict:\n",
    "            paired_images.append((view_dict['L-CC.png'], view_dict['L-MLO.png']))\n",
    "            new_labels.append(file_path_to_label[view_dict['L-CC.png']])\n",
    "            \n",
    "            paired_images.append((view_dict['L-MLO.png'], view_dict['L-CC.png']))\n",
    "            new_labels.append(file_path_to_label[view_dict['L-CC.png']])\n",
    "            \n",
    "        if 'R-CC.png' in view_dict and 'R-MLO.png' in view_dict:\n",
    "            paired_images.append((view_dict['R-CC.png'], view_dict['R-MLO.png']))\n",
    "            new_labels.append(file_path_to_label[view_dict['R-CC.png']])\n",
    "            \n",
    "            paired_images.append((view_dict['R-MLO.png'], view_dict['R-CC.png']))\n",
    "            new_labels.append(file_path_to_label[view_dict['R-CC.png']])\n",
    "    else:\n",
    "        print(view_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for mammography images\n",
    "class MammographyDataset(Dataset):\n",
    "    def __init__(self, image_pairs, labels, transform=None):\n",
    "        self.image_pairs = image_pairs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path1, img_path2 = self.image_pairs[idx]\n",
    "        img1 = Image.open(img_path1).convert('RGB')\n",
    "        img2 = Image.open(img_path2).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return (img1, img2), label\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs, test_pairs, train_labels, test_labels = train_test_split(paired_images, new_labels, test_size=0.2, random_state=42, stratify=True)\n",
    "\n",
    "train_dataset = MammographyDataset(train_pairs, train_labels, transform=transform)\n",
    "test_dataset = MammographyDataset(test_pairs, test_labels, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetFC(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(ResNetFC, self).__init__()\n",
    "        self.base_model = models.resnet50(pretrained=True)\n",
    "        self.fc_in_features = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Identity() \n",
    "        self.fc1 = nn.Linear(self.fc_in_features, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, num_classes)\n",
    "\n",
    "    def forward(self, img):\n",
    "        feat = self.base_model(img)\n",
    "        output = self.fc1(feat)\n",
    "        output = self.fc2(output)\n",
    "        output = self.fc3(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\");\n",
    "model = ResNetFC()\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "kl_div = KLDivLoss(reduction = \"batchmean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'architecture': 'feedforward',\n",
    "    'lr': 0.0001,\n",
    "    'scheduler_factor': 0.5,\n",
    "    'scheduler_patience': 2,\n",
    "    'scheduler_min_lr': 1e-6,\n",
    "    'epochs': 40\n",
    "}\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    'min',\n",
    "    factor=config['scheduler_factor'],\n",
    "    patience=config['scheduler_patience'],\n",
    "    min_lr=config['scheduler_min_lr']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "min_loss = 100\n",
    "for epoch in tqdm_notebook(range(config['epochs'])):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for (images1, images2), labels in train_dataloader:\n",
    "        images1, images2, labels = images1.to(device), images2.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out1 = model(images1)\n",
    "        out2 = model(images2)\n",
    "        loss1 = criterion(out1, labels)\n",
    "        \n",
    "        kl_loss1 = F.kl_div(F.log_softmax(out2, dim=1), F.softmax(out1, dim=1), reduction='batchmean')\n",
    "        kl_loss2 = F.kl_div(F.log_softmax(out1, dim=1), F.softmax(out2, dim=1), reduction='batchmean')\n",
    "        kl_loss = (kl_loss1 + kl_loss2)/2\n",
    "        \n",
    "        total_loss = loss1 + kl_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "        \n",
    "    print(f\"Epoch [{epoch + 1}/{config['epochs']}], Train Loss: {running_loss / len(train_dataloader):.6f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (images1, images2), labels in test_dataloader:\n",
    "            images1, images2, labels = images1.to(device), images2.to(device), labels.to(device)\n",
    "            out1 = model(images1)\n",
    "            out2 = model(images2)\n",
    "            loss1 = criterion(out1, labels)\n",
    "            \n",
    "            kl_loss1 = F.kl_div(F.log_softmax(out2, dim=1), F.softmax(out1, dim=1), reduction='batchmean')\n",
    "            kl_loss2 = F.kl_div(F.log_softmax(out1, dim=1), F.softmax(out2, dim=1), reduction='batchmean')\n",
    "            kl_loss = (kl_loss1 + kl_loss2)/2\n",
    "            \n",
    "            total_loss = loss1 + kl_loss\n",
    "            test_loss += total_loss.item()\n",
    "        scheduler.step(test_loss)\n",
    "           \n",
    "    print(f\"Test Loss: {test_loss / len(test_dataloader):.6f}\")\n",
    "               \n",
    "    if test_loss < min_loss:\n",
    "        state_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'train_loss': running_loss,\n",
    "            'val_loss': test_loss,\n",
    "            'best_val_loss': min_loss,\n",
    "        }\n",
    "        weight_path = f\"<your_dir>/resnet50_pretrained_paired_aux_{epoch}.pth\"\n",
    "        torch.save(model.state_dict(), weight_path)\n",
    "        min_loss = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNetFC()\n",
    "weight_path = '<your_best_weight_path>'\n",
    "model.load_state_dict(torch.load(weight_path))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "cm_labels = []\n",
    "cm_preds = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        image1, labels = images[0].to(device), labels.to(device)\n",
    "        outputs = model(image1)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        cm_preds += list(preds.cpu().numpy())\n",
    "        cm_labels += list(labels.cpu().numpy())\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "print(f\"Test Loss: {test_loss / len(test_dataloader):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor, mean, std):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_plot(image_tensors, single_label):\n",
    "    input_tensor1 = image_tensors[0].unsqueeze(0).cuda()  # Add batch dimension if not already present\n",
    "    input_tensor2 = image_tensors[1].unsqueeze(0).cuda()  # Add batch dimension if not already present\n",
    "    target_layers = [model.base_model.layer4[-1]] # Define the target layers and create the GradCAM object\n",
    "    cam = GradCAM(model=model, target_layers=target_layers)\n",
    "    targets = [ClassifierOutputTarget(single_label)] # Define the target class for Grad-CAM\n",
    "    grayscale_cam1 = cam(input_tensor=input_tensor1, targets=targets)[0, :] # Generate the CAM\n",
    "    grayscale_cam2 = cam(input_tensor=input_tensor2, targets=targets)[0, :] # Generate the CAM\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    denormalized_tensor1 = denormalize(image_tensors[0].clone(), mean, std)\n",
    "    denormalized_tensor2 = denormalize(image_tensors[1].clone(), mean, std)\n",
    "    rgb_img1 = denormalized_tensor1.permute(1, 2, 0).cpu().numpy()  # Convert CHW to HWC format\n",
    "    rgb_img1 = (rgb_img1 * 255).astype(np.uint8)  # Scale to [0, 255] and convert to uint8 for visualization\n",
    "    rgb_img2 = denormalized_tensor2.permute(1, 2, 0).cpu().numpy()  # Convert CHW to HWC format\n",
    "    rgb_img2 = (rgb_img2 * 255).astype(np.uint8)  # Scale to [0, 255] and convert to uint8 for visualization\n",
    "    visualization1 = show_cam_on_image(rgb_img1 / 255.0, grayscale_cam1, use_rgb=True)\n",
    "    visualization2 = show_cam_on_image(rgb_img2 / 255.0, grayscale_cam2, use_rgb=True)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs1 = model(input_tensor1)\n",
    "        _, predicted1 = torch.max(outputs1, 1)\n",
    "        predicted_label1 = predicted1.item()\n",
    "        \n",
    "        outputs2 = model(input_tensor2)\n",
    "        _, predicted2 = torch.max(outputs2, 1)\n",
    "        predicted_label2 = predicted2.item()\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(rgb_img1)\n",
    "    plt.title(f\"Original CC: BIRADS {single_label+1}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 2, 2) \n",
    "    plt.imshow(rgb_img2)\n",
    "    plt.title(f\"Original MLO: BIRADS {single_label+1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.imshow(visualization1)\n",
    "    plt.title(f\"Grad-CAM CC: BIRADS {predicted_label1+1}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.imshow(visualization2)\n",
    "    plt.title(f\"Grad-CAM MLO: BIRADS {predicted_label2+1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot up to 40 images, but feel free to adjust the number as needed.\n",
    "count = 0\n",
    "for img, label in test_dataset:\n",
    "    grad_plot(img, label)\n",
    "    count +=1\n",
    "    if count >=40:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix using seaborn\n",
    "cm = confusion_matrix(cm_labels, cm_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['1', '2', '3', '4', '5'],\n",
    "            yticklabels=['1', '2', '3', '4', '5'])\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "The input text\n",
    "text = \"\"\" Your model output, for example :\n",
    "Epoch [1/40], Train Loss: 1.367641, learning rate : 0.0001\n",
    "Test Loss: 1.330273\n",
    "Epoch [2/40], Train Loss: 0.992598, learning rate : 0.0001\n",
    "Test Loss: 1.575209\n",
    "Epoch [3/40], Train Loss: 0.595229, learning rate : 0.0001\n",
    "Test Loss: 1.421532\n",
    "\"\"\"\n",
    "\n",
    "# Regular expressions to extract epochs, train losses, and validation losses\n",
    "epoch_pattern = re.compile(r\"Epoch \\[(\\d+)/\\d+\\]\")\n",
    "train_loss_pattern = re.compile(r\"Train Loss: ([\\d\\.]+)\")\n",
    "val_loss_pattern = re.compile(r\"Test Loss: ([\\d\\.]+)\")\n",
    "\n",
    "# Extracting the data\n",
    "epochs = [int(epoch) for epoch in epoch_pattern.findall(text)]\n",
    "train_losses = [float(train_loss) for train_loss in train_loss_pattern.findall(text)]\n",
    "val_losses = [float(val_loss) for val_loss in val_loss_pattern.findall(text)]\n",
    "\n",
    "# Output the extracted data\n",
    "print(\"Epochs:\", epochs)\n",
    "print(\"Train Losses:\", train_losses)\n",
    "print(\"Validation Losses:\", val_losses)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_losses, label='Train Loss')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validation Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(all_preds, all_labels):\n",
    "    total_samples = len(all_labels)\n",
    "    correct_predictions = sum(1 for pred, label in zip(all_preds, all_labels) if pred == label)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return accuracy\n",
    "\n",
    "accuracy = calculate_accuracy(cm_preds, cm_labels)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
